---
title: "ML 1000 Assignment 2"
author: "by Anupama r.k, Queenie Tsang, Crystal (Yunan) Zhu"
date: "21/02/2021"
output:
  pdf_document: default
  html_document: default
---

## Abstract

Anomaly detection or Outlier detection identifies data points, events or observations that deviate from dataset's normal behavior. Anomalous data indicate critical incidents or potential opportunities. In order to take advantage of opportunities or fix costly problems anomaly detection has to be done in real time. Unsupervised machine learning models can be used to automate anomaly detection. Unsupervised anomaly detection algorithms scores data based on intrinsic properties of the dataset. Distances and densities are used to give an estimation what is normal and what is an outlier. Anomaly detection monitor is a tool developed for an online retailer to check product quality issues like profit opportunities and sales glitches. The application is built using R and Shinyapp following CRISP-DM framework. 


## Business Case



## Objective
Detect point anomalies from superstore dataset using K-NN and clustering methods. 

## Data Understanding

US Superstore dataset is sourced from [US uperstore dataset](https://www.kaggle.com/juhi1994/superstore) . The dataset have online orders for  Superstores in U.S. from 2014-2018. Tableau community is the owner of the dataset. The dataset has 9994 records and 21 attributes.

### Import data

```{r echo=FALSE,message=FALSE,warning=FALSE}


#load libraries
library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(anomalize)
library(lemon)
library(ggsci)
library(tidyverse)
library(ggplot2)    #for visualization
library(corrplot)   #for correlation matrix
library(Hmisc)      #for matrix of correlations and P-values
library(ggpubr)     #for making plots
library(factoextra) #for extracting and visualising results of multivariate data analyses 
library(cluster)
library(Rtsne)      #for t-SNE plot
library(DMwR)
library(outForest)
library(lubridate)
library(fpc)
library(FactoMineR)
library(NbClust)
library(RColorBrewer)
library(ggfortify)
```
```{r, echo= FALSE, message = FALSE, warning = FALSE}
superstore<- read_excel("US_Superstore_data.xls")
```
```{r include=FALSE}

#print dataset details
data_superstore<-as.data.frame( summary.default(superstore))
data_superstore<-data_superstore[-c(1:42),c(1,3)]
names(data_superstore)<-c("Attribute"," Data Type")
rownames(data_superstore)<-c()
data_superstore$Description<-c("row number","unique order number","order placed date","order shipping date","shipping mode of  order","unique customer id for order","name of customer","section of product","country based on order","city based on order","state based on order","pin code","region based on order","product id of  product","category of product","sub-category of product","name of product","selling price of product","order quantity","discount on product","profit from product")
knit_print.data.frame <- lemon_print

```

```{r caption ="Dataset description" ,render=lemon_print}
data_superstore
```





```{r, echo=FALSE,message=FALSE,warning=FALSE}

data=read.csv("US_Superstore_data.csv")
data$Order.Date=as.Date(data$Order.Date)
data$Ship.Date=as.Date(data$Ship.Date)

diff_in_days=as.numeric(data$Ship.Date-data$Order.Date)
data=cbind(data,diff_in_days)

#check dup
#data %>% distinct()
data_nodup=distinct(data,data[,1:21], keep_all=TRUE)[,-22]
#dim did not change - no dup

#check missing values
n=c()
for (i in 1:ncol(data)) {
  n[i]=sum(is.na(data[,i]))
}
missing_values=paste0(colnames(data),rep("-",ncol(data)),n,rep(" missing values",))
#cat("The number of missing values for each variable are:")
missing_values
#no missing values
data_miss=data[!complete.cases(data),]
```



Get a general idea of the data set.  


```{r}
length(unique(data$Customer.ID))
#793 unique customer IDs
length(unique(data$Customer.Name))
#793 unique customer names - drop one of these two vars


length(unique(data$Order.Date))
#1237 unique order dates
length(unique(data$Ship.Date))
#1334 unique ship dates - more unique ship dates than order dates - orders made on the same day were shipped in different dates??

length(unique(data$Segment))
unique(data$Segment)
#"Consumer"    "Corporate"   "Home Office"

unique(data$Country)
#all are from US - could drop this variable due to no-variation introduced by it

length(unique(data$City))
#531 different cities

length(unique(data$State))
#49 states

length(unique(data$Postal.Code))
#631 postal code - 793 unique customer IDs - some customers live very close!

unique(data$Region)
#only 4 regions

unique(data$Category)
#only 3 categories - "Furniture" "Office Supplies" "Technology"

length(unique(data$Sub.Category))
unique(data$Sub.Category)
#17 sub-categories 

length(unique(data$Product.Name))
#1850 product names
length(unique(data$Product.ID))
#1862 product IDs - potential redundant variables!

hist(data$diff_in_days)
#The time difference between order date and ship date typically takes 4 days. 

summary(data$Sales)
boxplot(data$Sales)
hist(data$Sales)
#a large amount of orders with very small Sales!


summary(data$Quantity)
boxplot(data$Quantity)
#not many outliers - the #of products in each order is stable?
hist(data$Quantity)
#very skewed distribution - most of the orders have small #of items

summary(data$Discount)
boxplot(data$Discount)
#a strange looking box dataplot? - median & 3rd quantile are the same (0.2) - not many orders have high discounts
hist(data$Discount)
#most of the orders were placed without any discounts or with 20% off

summary(data$Profit)
boxplot(data$Profit)
#most of the profits are outside of the box - but most of them clustered close to the box(not with so extreme values)
hist(data$Profit)
#most of the orders have profits ~1000 (or ~800?), and ~ -800
```


Remove the dot in the column names and replace with "_" to make variable names easier to handle:

```{r, echo=FALSE, warning=FALSE}
#replace . with _ in colnames
colnames(data) <- gsub("\\.", "_", colnames(data))
#check column names have been changed:
colnames(data)
```


# Exploratory Data Analysis  


Plot Sales in relation to Order Date:  


```{r, echo=FALSE, warning=FALSE}
ggplot(data = data) +
  geom_point(mapping = aes(x = Order_Date, y = Sales), xlab="Order Date", ylab="Sales")
```
Plot Profit in relation to Order Date:
```{r}
ggplot(data = data) +
  geom_point(mapping = aes(x = Order_Date, y = Profit), xlab="Order Date", ylab="Profit")
```
Some outliers for certain days


```{r}
table(data$`Sub_Category`)
```
look at the time range for these transactions, ie. start date for Order_Date column:

```{r}
summary(data$Order_Date)
#[1] min "2014-01-03", max "2017-12-30"
```
Basically this dataset covers transactions ranging from 2014-01-03 to 2017-12-30.


```{r}
ggplot(data = data) +
  geom_bar(mapping = aes(x = Category),fill="green4")
```
Most type of products sold belong to the Office supplies category.


```{r}
ggplot(data = data) +
  geom_bar(mapping = aes(y = `Sub_Category`), fill="green4")
```

```{r}
ggplot(data = data, mapping = aes(x = Sales)) +
  xlim(0, 5000) +
  geom_histogram(binwidth = 5,fill="green4")
```
Most sales are very few items (<500).

```{r}
ggplot(data = data, mapping = aes(x = Quantity)) +
  geom_histogram(binwidth = 0.5,fill="green4")
```


```{r}
ggplot(data = data) +
  geom_histogram(mapping = aes(x = Discount), 
                 binwidth = 0.05,
                 xlab="Discount",
                 fill="green4")
```
Sales transactions mostly do not involve discounts.


Visualise sales transactions by Region over time (order date).
```{r}
 ggplot(data, aes(Order_Date, Sales,color=Region)) +
      geom_line() 
```

Let's zoom in a little bit - Visualise sales transactions by Region over time (order date).

```{r}
 ggplot(data, aes(Order_Date, Sales,color=Region)) +
      geom_line() +
      ylim(0,5000)

```

How does profit change with sub-category?
```{r}
 #density plot where the count is standardized,area under each frequency is 1 
ggplot(data = data, mapping = aes(x = Sales, y = ..density..)) +   
  geom_freqpoly(mapping = aes(colour = Sub_Category), binwidth = 500)
```
It looks like some categories of items ie. supplies or accessories have negative sales values.


How does sales vary across sub category?
```{r}
ggplot(data = data, mapping = aes(x = Sales, y = `Sub_Category` )) +
  geom_boxplot()
```

```{r}
ggplot(data =data, mapping = aes(x = Ship_Mode)) +
  geom_bar()

```
Most transactions are shipped via Standard Class method.


```{r}
ggplot(data)+
geom_histogram(mapping=aes(x=Profit),fill="green3")+
coord_cartesian(ylim = c(0, 100))+
labs(title=" Profit Distribution")
```
```{r}
ggplot(data)+
geom_histogram(mapping=aes(x=Sales),fill="sienna3")+
coord_cartesian(ylim = c(0, 100))+labs(title=" Sales Distribution")
```
```{r}
ggplot(data) +
geom_point(mapping = aes(x = Profit, y = Discount),colour="violetred3")+
labs(title=" Profit Discount Distribution")
```
### Sales Profit
```{r}
ggplot(data) +
geom_point(mapping = aes(x = Sales, y = Profit),colour="limegreen")+
labs(title=" Sales Profit Distribution")
```
```{r}
#product name and product id mismatch
data %>% 
  distinct(Product_Name,Product_ID) %>% 
  group_by(Product_ID) %>% 
  filter(n()>1) %>% 
  select(Product_ID)

```


```{r}
#total category and subcategory 

count_category<-unique(data$Category)
length(count_category)


count_subcategory<-unique(data$Sub_Category)
length(count_subcategory)

data %>% 
  distinct(Category, Sub_Category)

```
```{r}

superstore_sales<-data %>% 
                  select(Order_Date,Sales)


superstore_sales<-as_tibble(superstore_sales)

```





Transactions by region:
```{r}
bar <- ggplot(data = data) + 
  geom_bar(
    mapping = aes(x = Region, fill = Region), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()
```
The above chart shows proportions of transactions from the different regions.


```{r, echo=FALSE, warning=FALSE}
#renaming Sub-Category to Sub_Category to avoid error with ggplot: 
names(data)[names(data)=="Sub-Category"] <- "Sub_Category"
```


```{r}
#Extracting the rows for South region, and sub-categories:
South <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "South")

bar <- ggplot(data = South) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```
In the South, most transactions are Binders, Paper, or Furnishings.

```{r}
#Extracting the rows for Central region, and sub-categories:
Central <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "Central")

bar <- ggplot(data = Central) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```

```{r}
#Extracting the rows for West region, and sub-categories:
West <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "West")

bar <- ggplot(data = West) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```

```{r}
#Extracting the rows for East region, and sub-categories:
East <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "East")

bar <- ggplot(data = East) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```


- **bar charts of profits/sales by region**  

```{r}
ggplot(data = data) + 
  geom_bar(mapping = aes(x = Region, fill = Sales)) +
  ggtitle("Total Sales by region") +
  ylab("Sales")

```
Total sales per region.




```{r}
ggplot(data = data) + 
  geom_bar(mapping = aes(x = Region, fill = Profit))+
  ggtitle("Total Profit by region")+
  ylab("Profit")
```


Look at relationship between numeric variables: 

```{r}
#subset the numeric variables:
numeric_vars<- c("Sales", "Quantity", "Discount", "Profit", "diff_in_days")
num_data <- data[numeric_vars]

```

We'll use a correlation matrix to look at the relationship between numeric variables:
```{r}
cor(num_data)
```
```{r}
#correlation matrix with statistical significance
cor_result=rcorr(as.matrix(num_data))

cor_result$r
corrplot(cor_result$r, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)  #display only upper triangle of correlation matrix

```

Discount is negatively correlated with profit, whereas sales is positively correlated with profit. The time between order date and
ship date (diff_in_days) is not correlated with sales, quantity, discount, or profit.

Since the difference in days between Order date and Ship date has 0 correlation with the other variables, let's drop diff_in_days 
for the K-means clustering analysis. 


## Data Preparation


```{r}
#make a copy of the original dataset and copy to data1
data1 <- data
```

drop column Row ID because it is not necessary; it is the row number from the original excel file. The country variable is also not needed because all the values are United states. 
Customer_Name and Customer_ID give redundant information. So we will drop the Customer_Name column and keep only the Customer_ID column.
```{r}
data1[,c("Row_ID","ï__Row_ID", "Country", "Customer_Name")]<-NULL
```



```{r}
head(data1)

```




## Model



For this K-means clustering we will use the numeric variables only: which are sales, quantity, discount, profit (columns 15 - 18). K means clustering is partitional, exclusive and complete type of clustering approach, meaning the clustering is independent, (not nested within each other), and every observation has to be placed inside a cluster (Nwanganga and Chapple, 2020).
K means clustering is affected by the starting assignment points, so we will try with 25 different starting assignments (nstart = 25), and see which ones work the best.



```{r, echo= FALSE, message=FALSE, warning = FALSE}
#Compute K-means clustering with k=3 (3 initial distinct cluster centres)
set.seed(123)

results_kmeans <- kmeans(scale(data1[,(15:18)]), 3, nstart =25)
                         
#kmeans clusters to show the group of the individuals
#results_kmeans$cluster
```

```{r}
summary(results_kmeans)

```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#results_kmeans  #this is commented out to prevent long print out

# https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967

#cluster means are the centroid vectors
#clustering vector is the group that the observation is placed into
#percentage indicates compactness of the clustering or how similar observations are within the same group
```

K-means clustering with 3 clusters of sizes 1136, 8831, 27

Cluster means:
        Sales     Quantity   Discount     Profit
1 -0.05714414  0.045908572  2.3730228 -0.5928228
2 -0.02922217 -0.007823215 -0.3039892  0.0425665
3 11.96210167  0.627210176 -0.4157497 11.0200717

Within cluster sum of squares by cluster:
[1]  5207.851 16648.669  3315.203
 (between_SS / total_SS =  37.0 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      

The results of this clustering indicate that the within cluster sum of squares by cluster is 37.0 % which means that the observations
within a given group are not very similar to each other. 

# Plot K-means

The factoextra package contains a function called fviz_cluster() which can be used to visualize kmeans clusters. The input required is the original dataset, and the kmeans results. These are used to produce plots which show points that represent observations.

```{r}
fviz_cluster(results_kmeans, data = data1[,(15:18)],
             palette = c("#2E9FDF", "#E495A5", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )

```

Reduce dimensions using Principal Component Analysis.

```{r}
results_pca <- prcomp(data1[,(15:19)], scale=TRUE)

#Coordinates of individual observations
indiv_coordinates <- as.data.frame(get_pca_ind(results_pca)$coord)

#Add clusters obtained through the Kmeans algorithm
indiv_coordinates$cluster <- factor(results_kmeans$cluster)

#Add region from the dataset
indiv_coordinates$Region <- data1$Region

#look at the first few rows of individual coordinates
head(indiv_coordinates)
```

Percentage of variance explained by dimensions.

```{r}
eigenvalue <- round(get_eigenvalue(results_pca), 1)
variance.percent <- eigenvalue$variance.percent
head(eigenvalue)

```

Variance of a group indicates how different members of a group are. Higher variance means greater dissimilarity within a group.

```{r}
#To visualize the k-means clusters:

ggscatter(
  indiv_coordinates, x = "Dim.1", y = "Dim.2",
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",    #adding the concentration ellipses
  shape = "Region", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  stat_mean(aes(color = cluster), size = 4)      #stat_mean is used for adding the cluster centroid
```

The clustering plot shows that the groups are very close together, and overlap slightly. The clusters could be further apart with some tuning by changing the number of clusters (k).


## Evaluation

(https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)

The within sum of squares (Withinss) is a value that represents the level of dissimilarity within a group. The higher the withinss, the greater the dissimilarity within the group.

(Foncesca, 2019)
```{r, warning = FALSE}
#To plot a within sum of squares plot for a range of different number of initial K-means centroids:

#This function is from:  (https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)

#data is the input dataset, nc is the maximum number of initial centres

wssplot <- function(data, nc=25, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of groups",
                     ylab="Sum of squares within a group")}

wssplot(data1[,(15:18)], nc = 20)

```

From the Within sum of squares plot, the optimal number of clusters is around 5. when the number of groups (k) initially increases 1 to 6, 
the error measures (sum of squares within a group) starts to decrease. When the number of groups is 7 or 8, the error measure starts to flatten. 

The main purpose is to find a number of initial groups which achieves some fair amount of compactness (or similarity) between observations within a group. When k is too high, each cluster starts to represent individual points, whereas when k is too low, the observations may not 
be in the right cluster. 


We can try re-running the k-means model with the number of groups, k  = 4

```{r, message=FALSE, warnings=FALSE, echo=FALSE}
set.seed(123)
clustering_results_4 <- kmeans(scale(data1[,(15:18)]), centers = 4, nstart = 25)
#clustering_results_4

```
K-means clustering with 4 clusters of sizes 27, 1044, 2838, 6085

Cluster means:
       Sales    Quantity   Discount      Profit
1 11.9621017  0.62721018 -0.4157497 11.02007172
2 -0.1222964  0.02095769  2.4788189 -0.58466503
3  0.2856822  1.19688927 -0.2981228  0.17558930
4 -0.1653353 -0.56459922 -0.2844025 -0.03048054

Within cluster sum of squares by cluster:
[1] 3315.203 4284.643 7571.058 3614.724
 (between_SS / total_SS =  53.0 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"  

The within cluster sum of squares by cluster value is now 53.0%. This represents the compactness of the clustering, or how similar observations are to other observations within the same group. 

```{r}
# Dimension reduction using PCA
results_pca_4 <- prcomp(data1[,(15:18)],  scale = TRUE)

# Coordinates of individuals
ind.coord <- as.data.frame(get_pca_ind(results_pca_4)$coord)

# Add clusters obtained using the K-means algorithm
ind.coord$cluster <- factor(clustering_results_4$cluster)

# Add Region groups from the original data sett
ind.coord$Region <- data1$Region

#look at the first few rows to double check
head(ind.coord)
```

```{r}
# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(results_pca_4), 1)
variance.percent <- eigenvalue$variance.percent
head(eigenvalue)
```

```{r}
ggscatter(
  ind.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = brewer.pal(n = 8, name = "Dark2"), ellipse = TRUE, ellipse.type = "convex",   #add concentration ellipses
  shape = "Region", size = 1.5,  legend = "right", ggtheme = theme_bw(),                #change point shapes depending on Region values
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  stat_mean(aes(color = cluster), size = 4)      #add cluster centroid using stat_mean()

```

# Clustering Validation


(https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)


Silhouette coefficient can be used to evaluate the goodness of the clustering. First, for each observation i, it calculates the dissimilarity
between i and all the other points within the same cluster. This value is called average dissimilarity Di.

Then calculate the dissimilarity between i and all the other clusters and get the lowest value among them. Find the dissimilarity between i 
and the next closest cluster, called Ci.

Next find the silhouette width which is the difference between Ci and Di, divided by the maximum difference between Ci and Di. 
Si = (Ci - Di)/max(Di, Ci)

Si>0 means the observation is well clustered. The closer it is to 1 the better it is clustered.

Si <0 means the observation is wrongly clustered.

Si = 0 means the observation is between 2 clusters. 

```{r}
library(cluster)
library(factoextra)

sil <- silhouette(clustering_results_4$cluster, dist(data1[,(15:18)]))
fviz_silhouette(sil)
```

We can try re-running the k-means model with the number of groups, k  = 7

```{r}
set.seed(123)
clustering_results_7 <- kmeans(scale(data1[,(15:18)]), centers = 7, nstart = 25)
#clustering_results_7

```
K-means clustering with 7 clusters of sizes 3317, 200, 12, 914, 2574, 9, 2968

Cluster means:
        Sales    Quantity   Discount       Profit
1 -0.17099202 -0.55401551 -0.7476979   0.01999970
2  3.70037129  1.13047292 -0.4332859   2.62598142
3  8.36049519  1.14320638  2.0689103 -11.91366497
4 -0.21824012  0.05129912  2.6451425  -0.46197164
5  0.04223395  1.22666437 -0.2946436   0.03793345
6 16.87915324  0.84359568 -0.6489669  19.79865024
7 -0.11265862 -0.54381990  0.2993725  -0.10180518

Within cluster sum of squares by cluster:
[1] 1131.9138 1960.3599 1460.2766 1749.2333 3515.6513  778.7336 1329.8103
 (between_SS / total_SS =  70.2 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"    


We also re-ran kmeans with k=8, 
Re-running k-means with k = 8:

The within cluster sum of squares by cluster value is 73.1% for k= 8, which is not very different from the Within cluster sum of square by cluster value for k = 7 (70.2%).

Compute PCA and extract individual components and extract individual components.



### Local Outlier Factor Algorithm  -Nearest neighbour method 

LOF uses density based methods to calculate degree of outlying. LOF is a unsupervised anomaly detection technique, every point in the dataset is assigned LOF score based on the threshold value it classify the data points as outlier or non-outlier.

Points which are much lower in density compared to its neighbours are considered outliers. 

A LOF score of approximately 1 indicates that density around the point is comparable to its neighbors. Scores significantly larger than 1 indicate outliers (Hahsler).

If there are more than k duplicate points in the data, then lof can become NaN caused by an infinite local density. In this case we set lof to 1
 
```{r}

#remove duplicates rows 
superstore_unq<-data1[!duplicated(data1[c("Sales","Profit","Quantity","Discount")]),]

#select numerical variables
superstore_lof<-superstore_unq[,c("Sales","Profit","Quantity","Discount")]

# for k=10, ,lof produces a vector of local outlier factor for each data point
lof_scores <- lofactor(superstore_lof, k=10)
plot(density(lof_scores))
 
# distribution of outlier factors
summary(lof_scores)
hist(lof_scores, breaks=10)

```


### Manual Evaluation

```{r}
#top 5 outliers transactons , the greater the lof_scores the farther the point is from the cluster. Choosing values for score greater than 2
 lof_outliers <- order(lof_scores>2, decreasing=T)[1:13]
 superstore_unq[lof_outliers,]
```


```{r}
#outliers for z-score >2(how far an observation is from the mean)

outlier_orderid<-superstore_unq[which(lof_scores >2),1]
vec<-as.vector(outlier_orderid)


#new column for outlier status
superstore_unq<-mutate(superstore_unq,outlier_status=ifelse(Order_ID %in% (vec) ,"Yes","No"))

x<-subset(superstore_unq,superstore_unq$outlier_status=="Yes")
head(x, 10)          #look at top 10 rows of x

```

Keeping just the normal transactions of the superstore dataset (which are not outliers):
```{r}
normal_transactions <-subset(superstore_unq,superstore_unq$outlier_status=="No")

```

Extract just the numeric columns from the normal transactions:
```{r}
normal_transactions_numeric <- normal_transactions %>% 
                        select(Sales, Quantity, Discount, Profit)
```

First scale the data since the variables are measured in different scales:
```{r}
normal_transactions_scaled <- scale(normal_transactions_numeric)

rownames(normal_transactions_scaled) <- normal_transactions_numeric$name     #reassign the rownames
```

Use PCA to scale the dimensions of the data:
```{r}
res.pca <- PCA(normal_transactions_scaled, graph = FALSE)

#Visualize eigenvalues/variances
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
```
Scree plot shows dimension 1 captures 44.8% of the variance while dimension 2 captures 25.4% of the variance.

Extract the results for variables
```{r}
var <- get_pca_var(res.pca)
```

Contributions of variables to PC1
```{r}
fviz_contrib(res.pca, choice = "var", axis = 1, top = 10)
```
Contributions of variables to PC2
```{r}
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
```

#Control variable colours using their contributions to the principal axis
```{r}
fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols =c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE #prevent text overlapping
             ) + theme_minimal() + ggtitle("Variables - PCA")
```
From the above visualization, sales and profit tend to increase together. Quantity also increases as sales and profits increase.
Discount is inversely related to sales, profit and quantity.




Using the Elbow method to determine the optimal number of clusters involves plotting the within cluster sum of squares. The optimal number of clusters is the one located at the "bend" on the graph, where WSS is lowered. Within-cluster sum of squares (WCSS) is the degree to which items within a cluster are similar or dissimilar (Nwanganga and Chapple, 2020).

The closer the items within a cluster are to the centroid, the lower the value of WCSS. 
```{r}
set.seed(123)

#to compute the total within-cluster sum of squares:
fviz_nbclust(normal_transactions_scaled, kmeans, method = "wss", k.max = 20) + 
  theme_minimal() + ggtitle("The Elbow Method")
```
The optimal number of clusters looks to be k = 4 (the elbow point). K=4 is where the curve bends, indicating the point at which increasing the number of clusters (k) does not significantly decrease the total within cluster sum of square value.  

Another method is to look at the silhouette method. The silhouette of an observation is a value that represents how closely the item is matched with other items with the the same cluster and how loosely matched it is with observations in a different cluster. 
```{r}
fviz_nbclust(normal_transactions_scaled, kmeans, method = "silhouette", k.max = 24) + theme_minimal() + ggtitle("The Silhouette Plot")
```
The silhouette method suggests the optimal number of clusters is 2, since k=2 corresponds to the highest average silhouette width value. 


A third statistical approach called the gap statistic method, compares the difference between clusters created from observed data and clusters created from a randomly generated dataset called the reference dataset.  The gap statistic for a given k value is the difference in the total WCSS for the observed data and that of the reference dataset. The optimal number of clusters is denoted by the k value that yields the largest gap statistic.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# compute gap statistic
set.seed(123)
gap_stat <- clusGap(normal_transactions_scaled, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```
Visualize the results of the gap statistic method.
```{r}
fviz_gap_stat(gap_stat)
```
Finding the optimal number of clusters using the gap statistic. 

```{r, echo= FALSE, message=FALSE, warning= FALSE}
#Compute K-means clustering with k=3 (3 initial distinct cluster centres) on just the normal transactions from the dataset. Nstart =25 means generate 25 initial configurations and find the average for all the centroids. 
set.seed(123)

results_kmeans <- kmeans(normal_transactions_scaled, centers = 3, nstart =25)
                         
#kmeans clusters to show the group of the individuals

#results_kmeans$cluster
```

```{r}
library(RColorBrewer)
fviz_cluster(results_kmeans, data = normal_transactions_scaled,
             palette = brewer.pal(n = 8, name = "Dark2"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```
To see the kmeans results:
```{r}
#results_kmeans

#the clustering vector information has been omitted for brevity.
```
K-means clustering with 3 clusters of sizes 131, 6484, 1022

Cluster means:
        Sales    Quantity   Discount        Profit
1  5.29934595  1.04564847 -0.4747453  4.5131872279
2 -0.08504992 -0.01836038 -0.3350941  0.0001464245
3 -0.13967774 -0.01754527  2.1868313 -0.5794294944

[1]  4677.307 10427.565  2891.185
 (between_SS / total_SS =  41.1 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      

The within cluster sum of squares by cluster is 41.1% for k = 3. 


try clustering with kmeans using k = 4
```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

results_kmeans_4 <- kmeans(normal_transactions_scaled, centers = 4, nstart =25)
                         
#kmeans clusters to show the group of the individuals

#results_kmeans_4
```
K-means clustering with 4 clusters of sizes 1741, 4917, 941, 38

Cluster means:
       Sales    Quantity   Discount     Profit
1  0.4660012  1.36028979 -0.3218513  0.2382839
2 -0.1953987 -0.47692843 -0.3185796 -0.0499454
3 -0.1892876 -0.04414503  2.2823862 -0.5695165
4  8.6207116  0.48244836 -0.5506917  9.6485345

Within cluster sum of squares by cluster:
[1] 5391.896 3363.327 2252.744 2215.088
 (between_SS / total_SS =  56.7 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      


The within sum of squares by cluster is 56.7%.


Plotting the 4 kmeans clustering results
```{r}
library(RColorBrewer)
fviz_cluster(results_kmeans_4, data = normal_transactions_scaled,
             palette = brewer.pal(n = 8, name = "Dark2"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```



### Method 2: K-medoids


Following this tutorial: https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3

K-means algorithm is limited in that it can only work with numerical data, whereas our dataset contains both numeric and categorical data. 

We will now try the PAM clustering algorithm (Partitioning across medoids).  K-medoids is more robust to outliers and noise than K-means. The reason for this is because K-medoids uses some of the observations as the cluster centres (medoids). This algorithm uses Gower distance to measure the partial dissimilarity across individuals, and ranges in [0 1]. Standardization is first applied to the features, and the distance
between individuals represents the average of all feature specific distances. 

Partial dissimilarity is different depending on the type of variable: numeric or categorical. 

Numeric features - partial dissimilarity is dependent on absolute difference between 2 observations (x_i and x_j), and the maximum
range observed from all individuals. d_ij^f = |x_i - x_j|/|max_N(x) - min_N(x))| where N is the number of individuals in a dataset.

Categorical/Qualitative features - feature dissimilarity is equal to 1 if y_i and y_j do not have the same values, otherwise it is 0.


One method to determine the number of clusters is by using the Silhouette coefficient. 

### Data Preparation 

For this K-medoids analysis we will omit the Region variable because there are only 4 values and the information is too broad.

We will try to cluster transactions according to the following features: 

```{r}
data2 <- data1 %>% 
              select(Ship_Mode, Segment, City, State, Sub_Category, diff_in_days, Sales, Quantity, Discount, Profit)

#convert all character data type to factor:
data2[sapply(data2, is.character)] <- lapply(data2[sapply(data2, is.character)], 
                                       as.factor)
```

Compute Gower distance 
```{r}
gower_dist <- daisy(data2, metric = "gower")

gower_mat <- as.matrix(gower_dist)
```

```{r}
#Print most similar transactions
data2[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), arr.ind = TRUE)[1, ], ]
```

```{r}
#Print most dissimilar transactions
data2[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), arr.ind = TRUE)[1, ], ]

```

Try to figure out the number of clusters to use by using the silhouette coefficient. Typically the number of clusters used is between 2 and 8. 

```{r}
sil_width <- c(NA)
for (i in 2:8){
  pam_fit <- pam(gower_dist, diss = TRUE, k = i )
  sil_width[i] <- pam_fit$silinfo$avg.width
}

plot(1:8, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:8, sil_width)
```
2 clusters has the highest silhouette width, while 4 has the second highest. Higher silhouette width is generally better.  2 Clusters may be too simple, so We will pick k = 4.

To look at a summary of each of the clusters:

```{r}
k <- 4
pam_fit_4 <- pam(gower_dist, diss = TRUE, k)
pam_results_4 <- data2 %>%
  mutate(cluster = pam_fit_4$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results_4$the_summary
```
The first cluster has the majority values: ship mode is Second Class , segment is Consumer , City is San Francisco/other, State is California/other, sub category is furnishings. The mean difference in days between order date and ship date is 2.358 days, the mean value for Sales is $216.38, the mean quantity is 3.699, the discount mean is $0.1113, profit mean is $32.

cluster 1 is made of second class shippping, Consumer segment x San Franciso x California x furnishings. 


For cluster 2, the ship mode is mostly standard class, segment is mostly consumer, city is mostly Los Angeles/other, State is mostly California/Other, subcategory is mostly Binders/other. The mean difference in days between order date and ship date is 4.7 days, the mean sales $225.91, mean quantity is 3.48, discount is 0.2148, and profit mean is $19.67.

Cluster 2 is made of standard class shipping x consumer segment x Los Angeles x California x Binders. 

For cluster 3, the main ship mode is First class, the segment is mostly Home Office, the most common city is Philadelphia/Other, the most common State is Pennsylvania/Other,  and the most common subcategory is paper/other. For numeric variables, the mean difference in days between order date and ship date is 4.04 days, the mean sales $216.94, mean quantity is 3.72, mean discount is 0.2065, and mean profit is $10.08. 

Cluster 3 is First class shipping x Home Office segment x Philadelphia x Pennsylvania x Paper. Cluster 3 has the lowest mean profit of the 4 clusters. 

For cluster 4, the main ship mode is First class, the most common segment is Corporate, New York City/Other is the most common city, New York State/Other is the most common state, and the most common subcategory is paper. For the numeric features, the mean difference in days between order date and ship date is 4.25 days, the mean sales is $253.84, and mean quantity is 3.83, mean discount is 0.086, mean profit is $49.05.

Cluster 4 is made of First Class shipping x Corporate segment x New York City x New York State x Paper. Cluster 4 has the highest mean sales amount and mean profit, with lowest mean discount. 


We can now visualise the clusters in lower dimensional space with tSNE (t-Distributed Stochastic Neighbor Embedding) which can be used for dimensionality reduction.

```{r}
#summary(pam_fit_4)
```
Medoids:
       ID     
[1,]  560  560
[2,]    9    9
[3,]  951  951
[4,] 6943 6943

Objective function:
    build      swap 
0.3215923 0.3208374 

Numerical information per cluster:
     size  max_diss   av_diss  diameter   separation
[1,] 2172 0.5369861 0.3495248 0.7527143 0.0004132536
[2,] 3458 0.6045552 0.3040315 0.8092413 0.0142857143
[3,] 1673 0.5553794 0.3280627 0.7789958 0.0142857143
[4,] 2691 0.5881813 0.3147869 0.8127438 0.0004132536

Isolated clusters:
 L-clusters: character(0)
 L*-clusters: character(0)
 
 Average silhouette width per cluster:
[1] 0.07665593 0.11233789 0.05559724 0.07000398
Average silhouette width of total data set:
[1] 0.08368581

Available components:
[1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"       "call"   


These were the observations used as the medoids for the pam fit with k=4: 
```{r}
data2[pam_fit_4$medoids, ]
```

Plotting the k-medoids with k=4. This method applies t-distributed stochastic neighborhood embedding (t-SNE) which enables visualization of several variables in a lower dimensional space.
```{r, fig.height=8, fig.width=6}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit_4$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster)) +
  labs(title = 't-SNE 2D Projections of k-medoid clusters') +
  theme(plot.title = element_text(hjust = 0.5))
```
Clusters still look like they have some overlap, but cluster 1 is pretty well defined. 



```{r}
input_data <- data2
```

We also tried using k =3 with the following re
Let's try using k=3:
```{r}
gower_dist_3 = daisy(input_data, metric = "gower", type = list(logratio = 3))
                  gower_mat = as.matrix(gower_dist_3)
                  pam_fit_3 = pam(gower_mat, k=3, diss=TRUE)
```


```{r}
#summary(pam_fit_3)
```

some information from the summary of pam fit for k=3:

Medoids:
     ID           
[1,] "399"  "399" 
[2,] "9"    "9"   
[3,] "7119" "7119"

Numerical information per cluster:
     size  max_diss   av_diss  diameter separation
[1,] 2361 0.6204122 0.3754422 0.7803200 0.01490751
[2,] 4281 0.6045552 0.3173436 0.8092413 0.01524512
[3,] 3352 0.5882532 0.3311972 0.8127438 0.01490751

Isolated clusters:
 L-clusters: character(0)
 L*-clusters: character(0)

Average silhouette width per cluster:
[1] 0.06840880 0.09434220 0.06023282
Average silhouette width of total data set:
[1] 0.07677532

Available components:
[1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"       "call"     


The average silhouette width of the total data set is 0.0767753. 

```{r}
pam_results_3 <- data2 %>%
  mutate(cluster = pam_fit_3$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results_3$the_summary
```


```{r, fig.height=8, fig.width=6}
tsne_obj <- Rtsne(gower_dist_3, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit_3$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster)) +
   labs(title = 't-SNE 2D Projections of k-medoid clusters') +
  theme(plot.title = element_text(hjust = 0.5))
```



We also tried k=5 for k-medoids clustering. For k= 5, The average silhouette width per cluster were:
[1] 0.05349182 0.10906323 0.04114758 0.08911682 0.07800729. The average silhouette width of the total data set for k=5 is 0.08049662. We also plotted the clustering result using the gower distance metric and t-SNE for dimensionality reduction. However, the plot shows there is still a great deal of overlap between the clusters. 


## Evaluating the consistency within Clusters of Data
Silhouette coefficient can be used to compare the average distance to observations within the same cluster, to the average distance to observations in other clusters. 

High silhouette coefficient means that the observation is well clustered, while a low silhouette coefficient may indicate outliers.


## Deployment



## Responsible ML Framework




## Conclusion




## References

https://www.datanovia.com/en/blog/k-means-clustering-visualization-in-r-step-by-step-guide/

https://www.tidymodels.org/learn/statistics/k-means/

https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967

https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc

https://www.analyticsvidhya.com/blog/2020/12/a-case-study-to-detect-anomalies-in-time-series-using-anomalize-package-in-r/

https://www.datanovia.com/en/lessons/k-medoids-in-r-algorithm-and-practical-examples/

https://www.rdocumentation.org/packages/dbscan/versions/1.1-5/topics/lof

https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92f

Nwanganga, F. and Chapple, M. (2020). Practical Machine Learning in R. Indianapolis, Indiana: Wiley & Sons. 