---
title: "ML 1000 Assignment 2"
author: "by Anupama r.k, Queenie Tsang, Crystal (Yunan) Zhu"
date: "21/02/2021"
output:
  pdf_document: default
  html_document: default
---
# To do list:  


- **Add Pie charts! - by sub_category, region**  # (done)


- **Create a Month variable - to see the change of sales/profits by month?**  


- **bar charts of profits/sales by region**  #(done)


- **Output the characteristics of the orders with the highest and lowest profits/sales - e.g. what made the order? when? bought what product? in which city/state/region? Any discount?**  


- **relationship between discount & sales, discount & profits, sales & profits, and the role of region?**  


- **from someone's analysis - there is no significant change between the four discount categories when it comes to Sales**  


- **sales/profits by month, rather than by date? color by region?**  






## Abstract

Anomaly detection or Outlier detection identifies data points, events or observations that deviate from dataset's normal behavior. Anomalous data indicate critical incidents or potential opportunities. In order to take advantage of opportunities or fix costly problems anomaly detection has to be done in real time. Unsupervised machine learning models can be used to automate anomaly detection. Unsupervised anomaly detection algorithms scores data based on intrinsic properties of the dataset. Distances and densities are used to give an estimation what is normal and what is an outlier. Anomaly detection monitor is a tool developed for an online retailer to check product quality issues like profit opportunities and sales glitches. The application is built using R and Shinyapp following CRISP-DM framework. 


## Business Case



## Objective
Detect point anomalies from superstore dataset using K-NN and clustering methods. 

## Data Understanding

US Superstore dataset is sourced from [US uperstore dataset](https://www.kaggle.com/juhi1994/superstore) . The dataset have online orders for  Superstores in U.S. from 2014-2018. Tableau community is the owner of the dataset. The dataset has 9994 records and 21 attributes.

### Import data

```{r echo=FALSE,message=FALSE,warning=FALSE}


#load libraries
library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(anomalize)
library(lemon)
library(ggsci)
library(tidyverse)
library(ggplot2)
library(corrplot)   #for correlation matrix
library(Hmisc)      #for matrix of correlations and P-values
library(ggpubr)     #for making plots
library(factoextra) #for extracting and visualising results of multivariate data analyses 
library(cluster)
library(Rtsne)
```
```{r}
superstore<- read_excel("US_Superstore_data.xls")
```
```{r include=FALSE}

#print dataset details
data_superstore<-as.data.frame( summary.default(superstore))
data_superstore<-data_superstore[-c(1:42),c(1,3)]
names(data_superstore)<-c("Attribute"," Data Type")
rownames(data_superstore)<-c()
data_superstore$Description<-c("row number","unique order number","order placed date","order shipping date","shipping mode of  order","unique customer id for order","name of customer","section of product","country based on order","city based on order","state based on order","pin code","region based on order","product id of  product","category of product","sub-category of product","name of product","selling price of product","order quantity","discount on product","profit from product")
knit_print.data.frame <- lemon_print

```

```{r caption ="Dataset description" ,render=lemon_print}
data_superstore
```





```{r, echo=FALSE,message=FALSE,warning=FALSE}

data=read.csv("US_Superstore_data.csv")
data$Order.Date=as.Date(data$Order.Date)
data$Ship.Date=as.Date(data$Ship.Date)

diff_in_days=as.numeric(data$Ship.Date-data$Order.Date)
data=cbind(data,diff_in_days)

#check dup
#data %>% distinct()
data_nodup=distinct(data,data[,1:21], keep_all=TRUE)[,-22]
#dim did not change - no dup

#check missing values
n=c()
for (i in 1:ncol(data)) {
  n[i]=sum(is.na(data[,i]))
}
missing_values=paste0(colnames(data),rep("-",ncol(data)),n,rep(" missing values",))
#cat("The number of missing values for each variable are:")
missing_values
#no missing values
data_miss=data[!complete.cases(data),]
```



Get a general idea of the data set.  


```{r}
length(unique(data$Customer.ID))
#793 unique customer IDs
length(unique(data$Customer.Name))
#793 unique customer names - drop one of these two vars


length(unique(data$Order.Date))
#1237 unique order dates
length(unique(data$Ship.Date))
#1334 unique ship dates - more unique ship dates than order dates - orders made on the same day were shipped in different dates??

length(unique(data$Segment))
unique(data$Segment)
#"Consumer"    "Corporate"   "Home Office"

unique(data$Country)
#all are from US - could drop this variable due to no-variation introduced by it

length(unique(data$City))
#531 different cities

length(unique(data$State))
#49 states

length(unique(data$Postal.Code))
#631 postal code - 793 unique customer IDs - some customers live very close!

unique(data$Region)
#only 4 regions

unique(data$Category)
#only 3 categories - "Furniture" "Office Supplies" "Technology"

length(unique(data$Sub.Category))
unique(data$Sub.Category)
#17 sub-categories 

length(unique(data$Product.Name))
#1850 product names
length(unique(data$Product.ID))
#1862 product IDs - potential redundant variables!

hist(data$diff_in_days)
#The time difference between order date and ship date typically takes 4 days. 

summary(data$Sales)
boxplot(data$Sales)
hist(data$Sales)
#a large amount of orders with very small Sales!


summary(data$Quantity)
boxplot(data$Quantity)
#not many outliers - the #of products in each order is stable?
hist(data$Quantity)
#very skewed distribution - most of the orders have small #of items

summary(data$Discount)
boxplot(data$Discount)
#a strange looking box dataplot? - median & 3rd quantile are the same (0.2) - not many orders have high discounts
hist(data$Discount)
#most of the orders were placed without any discounts or with 20% off

summary(data$Profit)
boxplot(data$Profit)
#most of the profits are outside of the box - but most of them clustered close to the box(not with so extreme values)
hist(data$Profit)
#most of the orders have profits ~1000 (or ~800?), and ~ -800
```


Remove the dot in the column names and replace with "_" to make variable names easier to handle:

```{r, echo=FALSE, warning=FALSE}
#replace . with _ in colnames
colnames(data) <- gsub("\\.", "_", colnames(data))
#check column names have been changed:
colnames(data)
```


# Exploratory Data Analysis  


Plot Sales in relation to Order Date:  


```{r, echo=FALSE, warning=FALSE}
ggplot(data = data) +
  geom_point(mapping = aes(x = Order_Date, y = Sales), xlab="Order Date", ylab="Sales")
```
Plot Profit in relation to Order Date:
```{r}
ggplot(data = data) +
  geom_point(mapping = aes(x = Order_Date, y = Profit), xlab="Order Date", ylab="Profit")
```
Some outliers for certain days


```{r}
table(data$`Sub_Category`)
```
look at the time range for these transactions, ie. start date for Order_Date column:

```{r}
summary(data$Order_Date)
#[1] min "2014-01-03", max "2017-12-30"
```
Basically this dataset covers transactions ranging from 2014-01-03 to 2017-12-30.


```{r}
ggplot(data = data) +
  geom_bar(mapping = aes(x = Category),fill="green4")
```
Most type of products sold belong to the Office supplies category.


```{r}
ggplot(data = data) +
  geom_bar(mapping = aes(y = `Sub_Category`), fill="green4")
```

```{r}
ggplot(data = data, mapping = aes(x = Sales)) +
  xlim(0, 5000) +
  geom_histogram(binwidth = 5,fill="green4")
```
Most sales are very few items (<500).

```{r}
ggplot(data = data, mapping = aes(x = Quantity)) +
  geom_histogram(binwidth = 0.5,fill="green4")
```


```{r}
ggplot(data = data) +
  geom_histogram(mapping = aes(x = Discount), 
                 binwidth = 0.05,
                 xlab="Discount",
                 fill="green4")
```
Sales transactions mostly do not involve discounts.


Visualise sales transactions by Region over time (order date).
```{r}
 ggplot(data, aes(Order_Date, Sales,color=Region)) +
      geom_line() 
```

Let's zoom in a little bit - Visualise sales transactions by Region over time (order date).

```{r}
 ggplot(data, aes(Order_Date, Sales,color=Region)) +
      geom_line() +
      ylim(0,5000)

```

How does profit change with sub-category?
```{r}
 #density plot where the count is standardized,area under each frequency is 1 
ggplot(data = data, mapping = aes(x = Sales, y = ..density..)) +   
  geom_freqpoly(mapping = aes(colour = Sub_Category), binwidth = 500)
```
It looks like some categories of items ie. supplies or accessories have negative sales values.


How does sales vary across sub category?
```{r}
ggplot(data = data, mapping = aes(x = Sales, y = `Sub_Category` )) +
  geom_boxplot()
```

```{r}
ggplot(data =data, mapping = aes(x = Ship_Mode)) +
  geom_bar()

```
Most transactions are shipped via Standard Class method.


```{r}
ggplot(data)+
geom_histogram(mapping=aes(x=Profit),fill="green3")+
coord_cartesian(ylim = c(0, 100))+
labs(title=" Profit Distribution")
```
```{r}
ggplot(data)+
geom_histogram(mapping=aes(x=Sales),fill="sienna3")+
coord_cartesian(ylim = c(0, 100))+labs(title=" Sales Distribution")
```
```{r}
ggplot(data) +
geom_point(mapping = aes(x = Profit, y = Discount),colour="violetred3")+
labs(title=" Profit Discount Distribution")
```
### Sales Profit
```{r}
ggplot(data) +
geom_point(mapping = aes(x = Sales, y = Profit),colour="limegreen")+
labs(title=" Sales Profit Distribution")
```
```{r}
#product name and product id mismatch
data %>% 
  distinct(Product_Name,Product_ID) %>% 
  group_by(Product_ID) %>% 
  filter(n()>1) %>% 
  select(Product_ID)

```


```{r}
#total category and subcategory 

count_category<-unique(data$Category)
length(count_category)


count_subcategory<-unique(data$Sub_Category)
length(count_subcategory)

data %>% 
  distinct(Category, Sub_Category)

```
```{r}

superstore_sales<-data %>% 
                  select(Order_Date,Sales)


superstore_sales<-as_tibble(superstore_sales)

```





Transactions by region:
```{r}
bar <- ggplot(data = data) + 
  geom_bar(
    mapping = aes(x = Region, fill = Region), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()
```
The above chart shows proportions of transactions from the different regions.


```{r, echo=FALSE, warning=FALSE}
#renaming Sub-Category to Sub_Category to avoid error with ggplot: 
names(data)[names(data)=="Sub-Category"] <- "Sub_Category"
```


```{r}
#Extracting the rows for South region, and sub-categories:
South <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "South")

bar <- ggplot(data = South) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```
In the South, most transactions are Binders, Paper, or Furnishings.

```{r}
#Extracting the rows for Central region, and sub-categories:
Central <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "Central")

bar <- ggplot(data = Central) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```

```{r}
#Extracting the rows for West region, and sub-categories:
West <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "West")

bar <- ggplot(data = West) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```

```{r}
#Extracting the rows for East region, and sub-categories:
East <- data %>% 
  select(Region, Sub_Category) %>%
  filter(Region == "East")

bar <- ggplot(data = East) + 
  geom_bar(
    mapping = aes(x = Sub_Category, fill = Sub_Category), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()

```


- **bar charts of profits/sales by region**  

```{r}
ggplot(data = data) + 
  geom_bar(mapping = aes(x = Region, fill = Sales)) +
  ggtitle("Total Sales by region") +
  ylab("Sales")

```
Total sales per region.




```{r}
ggplot(data = data) + 
  geom_bar(mapping = aes(x = Region, fill = Profit))+
  ggtitle("Total Profit by region")+
  ylab("Profit")
```


Look at relationship between numeric variables: 

```{r}
#subset the numeric variables:
numeric_vars<- c("Sales", "Quantity", "Discount", "Profit", "diff_in_days")
num_data <- data[numeric_vars]

```

We'll use a correlation matrix to look at the relationship between numeric variables:
```{r}
cor(num_data)
```
```{r}
#correlation matrix with statistical significance
cor_result=rcorr(as.matrix(num_data))

cor_result$r
corrplot(cor_result$r, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)  #display only upper triangle of correlation matrix

```

Discount is negatively correlated with profit, whereas sales is positively correlated with profit. The time between order date and
ship date (diff_in_days) is not correlated with sales, quantity, discount, or profit.

Since the difference in days between Order date and Ship date has 0 correlation with the other variables, let's drop diff_in_days 
for the K-means clustering analysis. 


## Data Preparation


```{r}
#make a copy of the original dataset and copy to data1
data1 <- data
```

drop column Row ID because it is not necessary; it is the row number from the original excel file. The country variable is also not needed because all the values are United states. 
Customer_Name and Customer_ID give redundant information. So we will drop the Customer_Name column and keep only the Customer_ID column.
```{r}
data1[,c("Row_ID","ï__Row_ID", "Country", "Customer_Name")]<-NULL
```



```{r}
head(data1)

```




## Model



For this K-means clustering we will use the numeric variables only: which are sales, quantity, discount, profit (columns 15 - 18). 
K means clustering is affected by the starting assignment points, so we will try with 25 different starting assignments (nstart = 25), and see which ones work the best.

(https://www.datanovia.com/en/blog/k-means-clustering-visualization-in-r-step-by-step-guide/)

```{r}
#Compute K-means clustering with k=3 (3 initial distinct cluster centres)
set.seed(123)

results_kmeans <- kmeans(scale(data1[,(15:18)]), 3, nstart =25)
                         
#kmeans clusters to show the group of the individuals
results_kmeans$cluster
```

```{r}
summary(results_kmeans)

```
```{r}
results_kmeans
# https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967

#cluster means are the centroid vectors
#clustering vector is the group that the observation is placed into
#percentage indicates compactness of the clustering or how similar observations are within the same group
```

The results of this clustering indicate that the within cluster sum of squares by cluster is 37.0 % which means that the observations
within a given group are not very similar to each other. 

# Plot K-means

The factoextra package contains a function called fviz_cluster() which can be used to visualize kmeans clusters. The input required is the original dataset, and the kmeans results. These are used to produce plots which show points that represent observations.

```{r}
fviz_cluster(results_kmeans, data = data1[,(15:18)],
             palette = c("#2E9FDF", "#E495A5", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )

```

Reduce dimensions using Principal Component Analysis.

```{r}
results_pca <- prcomp(data1[,(15:19)], scale=TRUE)

#Coordinates of individual observations
indiv_coordinates <- as.data.frame(get_pca_ind(results_pca)$coord)

#Add clusters obtained through the Kmeans algorithm
indiv_coordinates$cluster <- factor(results_kmeans$cluster)

#Add region from the dataset
indiv_coordinates$Region <- data1$Region

#look at the first few rows of individual coordinates
head(indiv_coordinates)
```

Percentage of variance explained by dimensions.

```{r}
eigenvalue <- round(get_eigenvalue(results_pca), 1)
variance.percent <- eigenvalue$variance.percent
head(eigenvalue)

```

Variance of a group indicates how different members of a group are. Higher variance means greater dissimilarity within a group.

```{r}
#To visualize the k-means clusters:

ggscatter(
  indiv_coordinates, x = "Dim.1", y = "Dim.2",
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",    #adding the concentration ellipses
  shape = "Region", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  stat_mean(aes(color = cluster), size = 4)      #stat_mean is used for adding the cluster centroid
```

The clustering plot shows that the groups are very close together, and overlap slightly. The clusters could be further apart with some tuning by changing the number of clusters (k).


## Evaluation

(https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)

The within sum of squares (Withinss) is a value that represents the level of dissimilarity within a group. The higher the withinss,
the greater the dissimilarity within the group.

(Foncesca, 2019)
```{r}
#To plot a within sum of squares plot for a range of different number of initial K-means centroids:

#This function is from:  (https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)

#data is the input dataset, nc is the maximum number of initial centres

wssplot <- function(data, nc=25, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of groups",
                     ylab="Sum of squares within a group")}

wssplot(data1[,(15:18)], nc = 20)

```

From the Within sum of squares plot, the optimal number of clusters is around 5. when the number of groups (k) initially increases 1 to 6, 
the error measures (sum of squares within a group) starts to decrease. When the number of groups is 7 or 8, the error measure starts to flatten. 

The main purpose is to find a number of initial groups which achieves some fair amount of compactness (or similarity) between observations within a group. When k is too high, each cluster starts to represent individual points, whereas when k is too low, the observations may not 
be in the right cluster. 


We can try re-running the k-means model with the number of groups, k  = 4

```{r}
set.seed(123)
clustering_results_4 <- kmeans(scale(data1[,(15:18)]), centers = 4, nstart = 25)
clustering_results_4

```
The within cluster sum of squares by cluster value is now 53.0%. This represents the compactness of the clustering, or how similar observations are to other observations within the same group. 

We can try re-running the k-means model with the number of groups, k  = 7

```{r}
set.seed(123)
clustering_results_7 <- kmeans(scale(data1[,(15:18)]), centers = 7, nstart = 25)
clustering_results_7

```
Re-running k-means with k = 8:
```{r}
set.seed(123)
clustering_results_8 <- kmeans(scale(data1[,(15:18)]), centers = 8, nstart = 25)
clustering_results_8

```
The within cluster sum of squares by cluster value is 73.1% for k= 8, which is not very different from the Within cluster sum of square by cluster value for k = 7 (70.2%).


Let's plot the K-means clusters

```{r}
library(RColorBrewer)
fviz_cluster(clustering_results_8, data = data1[,(15:18)],
             palette = brewer.pal(n = 8, name = "Dark2"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```
There are still some overlaps between cluster groups.



Compute PCA and extract individual components and extract individual components.

```{r}
# Dimension reduction using PCA
results_pca_8 <- prcomp(data1[,(15:18)],  scale = TRUE)

# Coordinates of individuals
ind.coord <- as.data.frame(get_pca_ind(results_pca_8)$coord)

# Add clusters obtained using the K-means algorithm
ind.coord$cluster <- factor(clustering_results_8$cluster)

# Add Region groups from the original data sett
ind.coord$Region <- data1$Region

#look at the first few rows to double check
head(ind.coord)
```

```{r}
# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(results_pca_8), 1)
variance.percent <- eigenvalue$variance.percent
head(eigenvalue)
```

```{r}
ggscatter(
  ind.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = brewer.pal(n = 8, name = "Dark2"), ellipse = TRUE, ellipse.type = "convex",   #add concentration ellipses
  shape = "Region", size = 1.5,  legend = "right", ggtheme = theme_bw(),                #change point shapes depending on Region values
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  stat_mean(aes(color = cluster), size = 4)      #add cluster centroid using stat_mean()

```

# Clustering Validation


(https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967)


Silhouette coefficient can be used to evaluate the goodness of the clustering. First, for each observation i, it calculates the dissimilarity
between i and all the other points within the same cluster. This value is called average dissimilarity Di.

Then calculate the dissimilarity between i and all the other clusters and get the lowest value among them. Find the dissimilarity between i 
and the next closest cluster, called Ci.

Next find the silhouette width which is the difference between Ci and Di, divided by the maximum difference between Ci and Di. 
Si = (Ci - Di)/max(Di, Ci)

Si>0 means the observation is well clustered. The closer it is to 1 the better it is clustered.

Si <0 means the observation is wrongly clustered.

Si = 0 means the observation is between 2 clusters. 


```{r}
library(cluster)
library(factoextra)

sil <- silhouette(clustering_results_8$cluster, dist(data1[,(15:18)]))
fviz_silhouette(sil)
```

From the table, 5 of the 8 clusters have a negative silhouette width which means that some observations may
be in the wrong cluster, so the clustering is not very good. 


### Method 2: K-medoids


Following this tutorial: https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3

K-means algorithm is limited in that it can only work with numerical data, whereas our dataset contains both numeric and categorical
data.

We will now try the PAM clustering algorithm (Partitioning across medoids).  K-medoids is more robust to outliers and noise than K-means. This algorithm uses Gower distance to measure the partial dissimilarity across individuals, and ranges in [0 1]. Standardization is first applied to the features, and the distance
between individuals represents the average of all feature specific distances. 

Partial dissimilarity is different depending on the type of variable: numeric or categorical. 

Numeric features - partial dissimilarity is dependent on absolute difference between 2 observations (x_i and x_j), and the maximum
range observed from all individuals. d_ij^f = |x_i - x_j|/|max_N(x) - min_N(x))| where N is the number of individuals in a dataset.

Categorical/Qualitative features - feature dissimilarity is equal to 1 if y_i and y_j do not have the same values, otherwise it is 0.


One method to determine the number of clusters is by using the Silhouette coefficient. 

### Data Preparation 

For this K-medoids analysis we will omit the Region variable because there are only 4 values and the information is too broad.

We will try to cluster transactions according to the following features: 

```{r}
data2 <- data1 %>% 
              select(Ship_Mode, Segment, City, State, Sub_Category, diff_in_days, Sales, Quantity, Discount, Profit)

#convert all character data type to factor:
data2[sapply(data2, is.character)] <- lapply(data2[sapply(data2, is.character)], 
                                       as.factor)
```

Compute Gower distance 
```{r}
gower_dist <- daisy(data2, metric = "gower")

gower_mat <- as.matrix(gower_dist)
```

```{r}
#Print most similar transactions
data2[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), arr.ind = TRUE)[1, ], ]
```

```{r}
#Print most dissimilar transactions
data2[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), arr.ind = TRUE)[1, ], ]

```

Try to figure out the number of clusters to use by using the silhouette coefficient. Typically the number of clusters used is between 2 and 8. 

```{r}
sil_width <- c(NA)
for (i in 2:8){
  pam_fit <- pam(gower_dist, diss = TRUE, k = i )
  sil_width[i] <- pam_fit$silinfo$avg.width
}

plot(1:8, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:8, sil_width)
```
2 clusters has the highest silhouette width, while 4 has the second highest. 2 Clusters may be too simple, so We will pick k = 4.

To look at a summary of each of the clusters:

```{r}
k <- 4
pam_fit_4 <- pam(gower_dist, diss = TRUE, k)
pam_results_4 <- data2 %>%
  mutate(cluster = pam_fit_4$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results_4$the_summary
```
The first cluster has the majority values: ship mode is Second Class , segment is Consumer , City is San Francisco/other, State is California/other, sub category is furnishings. The mean difference in days between order date and ship date is 2.358 days, the mean value for Sales is $216.38, the mean quantity is 3.699, the discount mean is $0.1113, profit mean is $32.

cluster 1 is made of second class shippping, Consumer segment x San Franciso x California x furnishings. 


For cluster 2, the ship mode is mostly standard class, segment is mostly consumer, city is mostly Los Angeles/other, State is mostly California/Other, subcategory is mostly Binders/other. The mean difference in days between order date and ship date is 4.7 days, the mean sales $225.91, mean quantity is 3.48, discount is 0.2148, and profit mean is $19.67.

Cluster 2 is made of standard class shipping x consumer segment x Los Angeles x California x Binders. 

For cluster 3, the main ship mode is First class, the segment is mostly Home Office, the most common city is Philadelphia/Other, the most common State is Pennsylvania/Other,  and the most common subcategory is paper/other. For numeric variables, the mean difference in days between order date and ship date is 4.04 days, the mean sales $216.94, mean quantity is 3.72, mean discount is 0.2065, and mean profit is $10.08. 

Cluster 3 is First class shipping x Home Office segment x Philadelphia x Pennsylvania x Paper. Cluster 3 has the lowest mean profit of the 4 clusters. 

For cluster 4, the main ship mode is First class, the most common segment is Corporate, New York City/Other is the most common city, New York State/Other is the most common state, and the most common subcategory is paper. For the numeric features, the mean difference in days between order date and ship date is 4.25 days, the mean sales is $253.84, and mean quantity is 3.83, mean discount is 0.086, mean profit is $49.05.

Cluster 4 is made of First Class shipping x Corporate segment x New York City x New York State x Paper. Cluster 4 has the highest mean sales amount and mean profit, with lowest mean discount. 


We can now visualise the clusters in lower dimensional space with tSNE (t-Distributed Stochastic Neighbor Embedding) which can be used for dimensionality reduction.

```{r}
#summary(pam_fit_4)
```
Medoids:
       ID     
[1,]  560  560
[2,]    9    9
[3,]  951  951
[4,] 6943 6943

Objective function:
    build      swap 
0.3215923 0.3208374 

Numerical information per cluster:
     size  max_diss   av_diss  diameter   separation
[1,] 2172 0.5369861 0.3495248 0.7527143 0.0004132536
[2,] 3458 0.6045552 0.3040315 0.8092413 0.0142857143
[3,] 1673 0.5553794 0.3280627 0.7789958 0.0142857143
[4,] 2691 0.5881813 0.3147869 0.8127438 0.0004132536

Isolated clusters:
 L-clusters: character(0)
 L*-clusters: character(0)
 
 Average silhouette width per cluster:
[1] 0.07665593 0.11233789 0.05559724 0.07000398
Average silhouette width of total data set:
[1] 0.08368581

Available components:
[1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"       "call"   


```{r, fig.height=8, fig.width=6}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit_4$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```


```{r}
input_data <- data2
```


Let's try using k=3:
```{r}
gower_dist_3 = daisy(input_data, metric = "gower", type = list(logratio = 3))
                  gower_mat = as.matrix(gower_dist_3)
                  pam_fit_3 = pam(gower_mat, k=3, diss=TRUE)
```


```{r}
#summary(pam_fit_3)
```

some information from the summary of pam fit for k=3:

Medoids:
     ID           
[1,] "399"  "399" 
[2,] "9"    "9"   
[3,] "7119" "7119"

Numerical information per cluster:
     size  max_diss   av_diss  diameter separation
[1,] 2361 0.6204122 0.3754422 0.7803200 0.01490751
[2,] 4281 0.6045552 0.3173436 0.8092413 0.01524512
[3,] 3352 0.5882532 0.3311972 0.8127438 0.01490751

Isolated clusters:
 L-clusters: character(0)
 L*-clusters: character(0)

Average silhouette width per cluster:
[1] 0.06840880 0.09434220 0.06023282
Average silhouette width of total data set:
[1] 0.07677532

Available components:
[1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"       "call"      

```{r}
pam_results_3 <- data2 %>%
  mutate(cluster = pam_fit_3$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results_3$the_summary
```


```{r, fig.height=8, fig.width=6}
tsne_obj <- Rtsne(gower_dist_3, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit_3$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```



Let's try using k=5:
```{r}
gower_dist_5 = daisy(input_data, metric = "gower", type = list(logratio = 3))
                  gower_mat = as.matrix(gower_dist_5)
                  pam_fit_5 = pam(gower_mat, k=5, diss=TRUE)
```

```{r}
pam_results_5 <- data2 %>%
  mutate(cluster = pam_fit_5$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results_5$the_summary

```
```{r}
#summary(pam_fit_5)
```
Medoids:
     ID           
[1,] "399"  "399" 
[2,] "9"    "9"   
[3,] "3027" "3027"
[4,] "7230" "7230"
[5,] "7554" "7554"

    build      swap 
0.3108903 0.3097443 

Numerical information per cluster:
     size  max_diss   av_diss  diameter  separation
[1,] 1477 0.5268858 0.3377785 0.7446928 0.001792861
[2,] 3127 0.5648233 0.2947558 0.7741892 0.014292276
[3,] 1465 0.5496150 0.3155812 0.7789958 0.001792861
[4,] 1618 0.5867120 0.3260236 0.7515684 0.014292276
[5,] 2307 0.5948357 0.2969881 0.8127438 0.014472373

Isolated clusters:
 L-clusters: character(0)
 L*-clusters: character(0)

 
```Average silhouette width per cluster:
[1] 0.05349182 0.10906323 0.04114758 0.08911682 0.07800729
Average silhouette width of total data set:
[1] 0.08049662

Available components:
[1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"      
[9] "call"     





```{r, fig.height=8, fig.width=6}
tsne_obj <- Rtsne(gower_dist_5, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit_5$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

## Evaluating the consistency within Clusters of Data
Silhouette coefficient can be used to compare the average distance to observations within the same cluster, to the average distance to observations in other clusters. 

High silhouette coefficient means that the observation is well clustered, while a low silhouette coefficient may indicate outliers.


## Deployment



## Responsible ML Framework




## Conclusion




## References

https://www.datanovia.com/en/blog/k-means-clustering-visualization-in-r-step-by-step-guide/

https://www.tidymodels.org/learn/statistics/k-means/

https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967

https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc

https://www.analyticsvidhya.com/blog/2020/12/a-case-study-to-detect-anomalies-in-time-series-using-anomalize-package-in-r/

https://www.datanovia.com/en/lessons/k-medoids-in-r-algorithm-and-practical-examples/